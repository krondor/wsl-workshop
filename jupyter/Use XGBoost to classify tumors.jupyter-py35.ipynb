{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Use XGBoost to classify tumors with IBM Watson Machine Learning</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "   <tr style=\"border: none\">\n",
    "       <th style=\"border: none\"><img src=\"https://raw.githubusercontent.com/pmservice/wml-sample-notebooks/master/images/cancer_banner-06.png\" alt=\"Icon\" width=\"700\"> </th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains steps and code to get data from the IBM Watson Studio Community, create a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.5, XGBoost, and scikit-learn.\n",
    "\n",
    "You will use a publicly available data set, the Breast Cancer Wisconsin (Diagnostic) Data Set, to train an XGBoost Model to classify breast cancer tumors (as benign or malignant) from 569 diagnostic images based on measurements such as radius, texture, perimeter and area. XGBoost is short for “E**x**treme **G**radient **Boost**ing”.\n",
    "\n",
    "The XGBoost classifier makes its predictions based on the majority vote from collection of models which are a set of classification trees. It uses the combination of weak learners to create a single strong learner. It’s a sequential training process, whereby new learners focus on the misclassified examples of previous learners.\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "-  Load a CSV file into numpy array\n",
    "-  Explore data\n",
    "-  Prepare data for training and evaluation\n",
    "-  Create an XGBoost machine learning model\n",
    "-  Train and evaluate a model\n",
    "-  Use cross-validation to optimize model's hyperparameters\n",
    "-  Persist a model in Watson Machine Learning repository\n",
    "-  Deploy a model for online scoring\n",
    "-  Score sample data\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up the environment](#setup)\n",
    "2.\t[Load and explore the data](#load)\n",
    "3.\t[Create the XGBoost model](#model)\n",
    "4.\t[Persist model](#persistence)\n",
    "5.\t[Deploy and score in a Cloud](#scoring)\n",
    "6.\t[Summary and next steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 1. Set up the environment\n",
    "\n",
    "Before you use the sample code in this notebook, you have to perform the following setup tasks:\n",
    "\n",
    "- Create a [Watson Machine Learning (WML) Service](https://console.ng.bluemix.net/catalog/services/ibm-watson-machine-learning/) instance (a free plan is offered and information about how to create the instance is [here](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html))\n",
    "-  Download **Breast Cancer Wisconsin (Diagnostic) Data Set** dataset from Watson Studio [Community](https://dataplatform.ibm.com/community?context=analytics).\n",
    "\n",
    "**Note:** We provide the code to download data set, see [step 2](#load)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section you will load the data as a numpy array and perform a basic exploration.\n",
    "\n",
    "To load the data as a numpy array, user `wget` to download the data, then use the `genfromtxt` method to read the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Example**: First, you need to install the required packages. You can do this by running the following code. Run it only one time.<BR><BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install wget --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Insert Project Data Or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Fetch Remote Data\n",
    "import wget, os\n",
    "\n",
    "WisconsinDataSet = 'BreastCancerWisconsinDataSet.csv' \n",
    "if not os.path.isfile(WisconsinDataSet):\n",
    "    link_to_data = 'https://apsportal.ibm.com/exchange-api/v1/entries/c173693bf48aeb22e41bbe2b41d79c1f/data?accessKey=941eec501eadcdceb5abd25cf7c029d5'\n",
    "    WisconsinDataSet = wget.download(link_to_data)\n",
    "\n",
    "print(WisconsinDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The csv file **BreastCancerWisconsinDataSet.csv** is downloaded. Run the code in the next cells to load the file to the numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** Update `numpy` to ensure you have the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run this code to upgrade numpy.\n",
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np_data = np.genfromtxt(WisconsinDataSet, delimiter=',', names=True, dtype=None, encoding='utf-8')\n",
    "print(np_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the code in the next cell to view the feature names and data storage types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Display the feature names and data storage types.\n",
    "print(np_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Display the number of records and features.\n",
    "print('Number of rows: {}'.format(np_data.size))\n",
    "print('Number of columns: {}'.format(len(np_data[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that the data set has 569 records and 32 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create an XGBoost model\n",
    "\n",
    "In this section you will learn how to train and test an XGBoost model.\n",
    "\n",
    "- [3.1. Prepare the data](#prepare)\n",
    "- [3.2. Create the XGBoost model](#create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Prepare data<a id=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, you can prepare your data for model building. You will use the `diagnosis` column as your target variable so you must remove it from the set of predictors. You must also remove the `id` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y = 1.0*(np_data['diagnosis'] == 'M')\n",
    "X = np.array([list(r)[2:] for r in np_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split the data set into: \n",
    "- Train data set\n",
    "- Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split the data set and create two data sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List the number of records in each data set.\n",
    "print(\"Number of training records: \" + str(X_train.shape[0]))\n",
    "print(\"Number of testing records : \" + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data has been successfully split into two data sets:\n",
    "- The train data set, which is the largest group, will be used for training\n",
    "- The test data set will be used for model evaluation and is used to test the assumptions of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2. Create the XGBoost model<a id=\"create\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries you need to create the XGBoost model.\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2.1. Create an XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section you create an XGBoost classifier with default hyperparameter values and you will call it *xgb_model*. \n",
    "\n",
    "**Note** The next sections show you how to improve this base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create the XGB classifier, xgb_model.\n",
    "xgb_model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the default parameters for *xgb_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# List the default parameters.\n",
    "print(xgb_model.get_xgb_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that your XGBoost classifier, *xgb_model*, is set up, you can train it by invoking the fit method. You will also evaluate *xgb_model* while the train and test data are being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train and evaluate.\n",
    "xgb_model.fit(X_train, y_train, eval_metric=['error'], eval_set=[((X_train, y_train)),(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** You can also use a pandas dataFrame instead of the numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot the model performance evaluated during the training process to assess model overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot and display the performance evaluation\n",
    "xgb_eval = xgb_model.evals_result()\n",
    "eval_steps = range(len(xgb_eval['validation_0']['error']))\n",
    "\n",
    "fig, ax = pyplot.subplots(1, 1, sharex=True, figsize=(8, 6))\n",
    "\n",
    "ax.plot(eval_steps, [1-x for x in xgb_eval['validation_0']['error']], label='Train')\n",
    "ax.plot(eval_steps, [1-x for x in xgb_eval['validation_1']['error']], label='Test')\n",
    "ax.legend()\n",
    "ax.set_title('Accuracy')\n",
    "ax.set_xlabel('Number of iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that there is model overfitting, and there is a decrease in model accuracy after about 60 iterations \n",
    "\n",
    "Select the trained model obtained after 30 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select trained model.\n",
    "n_trees = 30\n",
    "y_pred = xgb_model.predict(X_test, ntree_limit= n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check the accuracy of the trained model.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.1f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Note:** You will use the accuracy value obtained on the test data to compare the accuracy of the model with default parameters to the accuracy of the model with tuned parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2.2. Use grid search and cross-validation to tune the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can use grid search and cross-validation to tune your model to achieve better accuracy.\n",
    "\n",
    "XGBoost has an extensive catalog of hyperparameters which provides great flexibility to shape an algorithm’s desired behavior. Here you will the optimize the model tuning which adds an L1 penalty (`reg_alpha`).\n",
    "\n",
    "Use a 5-fold cross-validation because your training data set is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the cell below, create the XGBoost pipeline and set up the parameter grid for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create XGBoost pipeline, set up parameter grid.\n",
    "xgb_model_gs = XGBClassifier()\n",
    "parameters = {'reg_alpha': [0.0, 1.0], 'reg_lambda': [0.0, 1.0], 'n_estimators': [n_trees], 'seed': [1337]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Use ``GridSearchCV`` to search for the best parameters over the parameters values that were specified in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Search for the best parameters.\n",
    "clf = GridSearchCV(xgb_model_gs, parameters, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, refit=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "From the grid scores, you can see the performance result of all parameter combinations including the best parameter combination based on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# View the performance result.\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the accuracy estimated using cross-validation and the hyperparameter values for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best score: %.1f%%\" % (clf.best_score_*100))\n",
    "print(\"Best parameter set: %s\" % (clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Display the accuracy of best parameter combination on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.1f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The accuracy on test set is about the same for tuned model as it is for the trained model that has default hyperparameters values, even though the selected hyperparameters are different to the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 3.2.3. Model with pipeline data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here you learn how to use the XGBoost model within the scikit-learn pipeline. \n",
    "\n",
    "Let's start by importing the required objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "xgb_model_pca = XGBClassifier(n_estimators=n_trees)\n",
    "pipeline = Pipeline(steps=[('pca', pca), ('xgb', xgb_model_pca)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you are ready to evaluate accuracy of the model trained on the reduced set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.1f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that this model has a similar accuracy to the model trained using default hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's see how you can save your XGBoost pipeline using the WML service instance and deploy it for online scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"persistence\"></a>\n",
    "## 4. Persist model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section you learn how to use the Python client libraries to store your XGBoost model in the WML repository.\n",
    "\n",
    "First, you must import client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save\n",
    "\n",
    "save(name='XGBoostTumorClassification',\n",
    "     model=pipeline,\n",
    "     x_test=pd.DataFrame(X_test),\n",
    "     y_test=pd.DataFrame(y_test),\n",
    "     algorithm_type='Classification',\n",
    "     source='Use XGBoost to classify tumors.ipynb',\n",
    "     description='Tumor Malignancy Classifiation with XGBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the saved model metadata from WML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Score the Staged Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section you will learn how to use WML to create online scoring and score a new data record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Perform prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, extract the url endpoint, *scoring_url*, which will be used to send scoring requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract endpoint url and display it.\n",
    "scoring_url = 'https://dsxl-api/v3/project/score/Python35/scikit-learn-0.20/wsl-workshop/XGBoostTumorClassification/1'\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prepare the scoring payload with the values to score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare scoring payload.\n",
    "json_payload = [{\"concave points_mean\":0.01171,\"perimeter_se\":1.115,\"fractal_dimension_mean\":0.05581,\"symmetry_se\":0.01619,\"smoothness_worst\":0.09616,\"concave points_se\":0.005905,\"fractal_dimension_se\":0.002081,\"concavity_se\":0.01652,\"compactness_se\":0.01345,\"compactness_mean\":0.03729,\"texture_mean\":13.12,\"radius_mean\":12.89,\"fractal_dimension_worst\":0.06915,\"area_mean\":515.9,\"radius_se\":0.1532,\"symmetry_worst\":0.2309,\"diagnosis\":\"B\",\"concave points_worst\":0.05366,\"area_worst\":577,\"perimeter_mean\":81.89,\"smoothness_se\":0.004731,\"texture_se\":0.469,\"area_se\":12.68,\"compactness_worst\":0.1147,\"symmetry_mean\":0.1337,\"smoothness_mean\":0.06955,\"perimeter_worst\":87.4,\"concavity_worst\":0.1186,\"concavity_mean\":0.0226,\"texture_worst\":15.54,\"radius_worst\":13.62,\"id\":8913}]\n",
    "\n",
    "payload_scoring = {\"values\": [X_test[0].tolist()]}\n",
    "print(payload_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform prediction and display the result.\n",
    "import requests, json, os\n",
    "\n",
    "header_online = {'Content-Type': 'application/json', 'Authorization':os.environ['DSX_TOKEN']}\n",
    "response_scoring = requests.post(scoring_url, json=json_payload, headers=header_online)\n",
    "response_scoring.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Result**: The patient record is classified as a benign tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 6. Summary and next steps     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You successfully completed this notebook! \n",
    "\n",
    "You learned how to use XGBoost machine learning as well as Watson Machine Learning to create and deploy a model. \n",
    "\n",
    "Check out our [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data citations\n",
    "\n",
    "Lichman, M. (2013). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright © 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
